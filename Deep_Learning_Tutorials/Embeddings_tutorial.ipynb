{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76456f1",
   "metadata": {},
   "source": [
    "Computational Linguistics &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; <br>Biao Yun\n",
    "\n",
    "<center> \n",
    "\n",
    "# Word Embeddings Assignment\n",
    "### 2023.03.20\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea2718",
   "metadata": {},
   "source": [
    "#### 引入所需 packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca7eef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ckiptagger import data_utils\n",
    "from ckiptagger import WS\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2970ad",
   "metadata": {},
   "source": [
    "#### 載入 CKIP 斷詞系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b30098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/biaoyun/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/Users/biaoyun/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ws = WS(\"/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddae13",
   "metadata": {},
   "source": [
    "## 讀入中文文件\n",
    "\n",
    "* 此檔案為華語男歌手 2013-2023 之間發布之歌曲歌詞，包含歌手、歌名、歌詞、網址與日期等欄位，由魔鏡歌詞網取得\n",
    "* 由於 CKIP 斷詞系統不堪負荷，從原檔案八萬多首歌中隨機取出5000首歌作為此次模型訓練資料\n",
    "* 模型分類目的為判斷是否為 positive or negative 的歌詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5558742f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/male_all_recent_10_years_song_lyrics_content_clean.csv', sep='\\t')\n",
    "df = df.iloc[29585:34585, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d648d223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singer</th>\n",
       "      <th>name</th>\n",
       "      <th>lyric</th>\n",
       "      <th>song_url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29585</th>\n",
       "      <td>高山林</td>\n",
       "      <td>五月花開</td>\n",
       "      <td>作詞：孫浩, 作曲：孫浩, 終於盼到了滿園春色遍地花開, 你說這個季節會回來, 滿天飛舞的花...</td>\n",
       "      <td>twy131563x1x4.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29586</th>\n",
       "      <td>高山林</td>\n",
       "      <td>兄弟保重</td>\n",
       "      <td>作詞：周兵, 作曲：石焱, 讓我這杯酒 為你送行, 太多的話 都在杯裡, 人生注定有短暫分離...</td>\n",
       "      <td>twy131563x1x3.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29587</th>\n",
       "      <td>高山林</td>\n",
       "      <td>你不懂愛</td>\n",
       "      <td>作詞：阿博, 作曲：阿博, 我問了很久, 是什麼原由, 讓我們走不到盡頭, 我等了很久, 你...</td>\n",
       "      <td>twy131563x1x5.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29588</th>\n",
       "      <td>高山林</td>\n",
       "      <td>乘風破浪</td>\n",
       "      <td>作詞：小白, 作曲：小白, 揚帆啟航 海岸天際, 仰望蒼穹 鳥瞰大地, 生命如歌 瑰麗如虹,...</td>\n",
       "      <td>twy131563x1x11.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29589</th>\n",
       "      <td>高山林</td>\n",
       "      <td>愛你必須的</td>\n",
       "      <td>作詞：周兵, 作曲：石焱, 回憶漸漸開始飄落, 牽過你的手還記得, 美夢簡單複製了快樂, 存...</td>\n",
       "      <td>twy131563x1x2.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      singer   name                                              lyric  \\\n",
       "29585    高山林   五月花開  作詞：孫浩, 作曲：孫浩, 終於盼到了滿園春色遍地花開, 你說這個季節會回來, 滿天飛舞的花...   \n",
       "29586    高山林   兄弟保重  作詞：周兵, 作曲：石焱, 讓我這杯酒 為你送行, 太多的話 都在杯裡, 人生注定有短暫分離...   \n",
       "29587    高山林   你不懂愛  作詞：阿博, 作曲：阿博, 我問了很久, 是什麼原由, 讓我們走不到盡頭, 我等了很久, 你...   \n",
       "29588    高山林   乘風破浪  作詞：小白, 作曲：小白, 揚帆啟航 海岸天際, 仰望蒼穹 鳥瞰大地, 生命如歌 瑰麗如虹,...   \n",
       "29589    高山林  愛你必須的  作詞：周兵, 作曲：石焱, 回憶漸漸開始飄落, 牽過你的手還記得, 美夢簡單複製了快樂, 存...   \n",
       "\n",
       "                 song_url     date  \n",
       "29585   twy131563x1x4.htm  2014-05  \n",
       "29586   twy131563x1x3.htm  2014-05  \n",
       "29587   twy131563x1x5.htm  2014-05  \n",
       "29588  twy131563x1x11.htm  2014-05  \n",
       "29589   twy131563x1x2.htm  2014-05  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6277c11",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "* 首先將歌詞欄位的「中文」提取出來（即刪掉英文或是其他語言之歌詞）\n",
    "* 接著將歌詞用 CKIP 進行斷詞（這邊真的跑超～～久，我原本想用一萬首，結果 kernel 當掉兩次😨只好換成五千首）\n",
    "* 再來引入 LIWC 繁體中文情緒詞典，對歌詞進行簡單的正負相情緒判別，產生 sentiment 欄位，即之後分類模型的 label\n",
    "* 正負向情緒給定方式為計算每首歌所含有之正向詞與負向詞，相減後大於 0 為正向，小於 0 為負向，等於 0 為中性\n",
    "* 為減少模型雜訊與負擔，將情緒為中性之歌詞刪除\n",
    "* 正向之歌詞給定 label 為 0，負向則為 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c964c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取中文字之 function\n",
    "\n",
    "def extract_chinese(text):\n",
    "    pattern = re.compile(\"[\\u4e00-\\u9fa5]\")\n",
    "    \n",
    "    return \"\".join(pattern.findall(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de94070",
   "metadata": {},
   "outputs": [],
   "source": [
    "ly_list = list(df['lyric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d7956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ly = []\n",
    "\n",
    "for i in range(len(ly_list)):\n",
    "    new_ly.append(extract_chinese(ly_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e95fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lyric_new'] = new_ly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a82805",
   "metadata": {},
   "source": [
    "#### 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47ae218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [38:12<00:00,  2.18it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_list = []\n",
    "for text in tqdm(new_ly):\n",
    "    tokens = ws([text])\n",
    "    token_list.append(tokens)\n",
    "# for text in df[\"lyric\"]:\n",
    "#   tokens = ws([text])\n",
    "#   token_list.append(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9aef2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flatten_tokens = []\n",
    "for list_1 in token_list:\n",
    "  for sublist in list_1:\n",
    "    flatten_tokens.append(sublist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6a474",
   "metadata": {},
   "source": [
    "#### 計算歌詞之情緒向度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f28c2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment\n",
    "\n",
    "LIWC = pd.read_csv(\"/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/LIWC.csv\", sep=',')\n",
    "\n",
    "pos = [31]\n",
    "neg = [32]\n",
    "for num_31 in pos:\n",
    "    pos_df = LIWC[LIWC.isin([num_31]).any(1)].reset_index(drop=True)\n",
    "for num_32 in neg:\n",
    "    neg_df = LIWC[LIWC.isin([num_32]).any(1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff7a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "negemo_list = []\n",
    "\n",
    "for neg in neg_df['%']:\n",
    "    negemo_list.append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab275a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posemo_list = []\n",
    "\n",
    "for pos in pos_df['%']:\n",
    "    posemo_list.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c98965c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "neutral = []\n",
    "for tokens in token_list:\n",
    "  pos_counter = 0\n",
    "  neg_counter = 0\n",
    "  neu = 0\n",
    "  for i in tokens[0]:\n",
    "    if i in posemo_list:\n",
    "      pos_counter = pos_counter+1\n",
    "  for i in tokens[0]:\n",
    "    if i in negemo_list:\n",
    "      neg_counter = neg_counter+1\n",
    "  if pos_counter == 0 and neg_counter == 0:\n",
    "    neu = 1\n",
    "  else:\n",
    "    new = 0\n",
    "  positive.append(pos_counter)\n",
    "  negative.append(neg_counter)\n",
    "  neutral.append(neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff57c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = []\n",
    "for tokens in token_list:\n",
    "  pos_count = 0\n",
    "  neg_count = 0\n",
    "  for i in tokens[0]:\n",
    "    if i in posemo_list:\n",
    "      pos_count = pos_count+1\n",
    "    if i in negemo_list:\n",
    "      neg_count = neg_count+1\n",
    "  if pos_count == 0 and neg_count == 0:\n",
    "    result = 2\n",
    "  elif pos_count-neg_count > 0:\n",
    "    result = 0\n",
    "  elif pos_count-neg_count < 0:\n",
    "    result = 1\n",
    "  emo.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f77a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56844e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_string = []\n",
    "\n",
    "for ft in flatten_tokens:\n",
    "    st = ' '.join(ft)\n",
    "    flatten_string.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4e4cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segmented_lyrics'] = flatten_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a60b0f",
   "metadata": {},
   "source": [
    "#### 刪除情緒為中性之歌詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de8e0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df.sentiment != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存檔案，才不會每次都要重新斷詞\n",
    "\n",
    "df_clean.to_csv('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Week_5/df_clean_new.csv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda4b65",
   "metadata": {},
   "source": [
    "## (1) Create a tf-idf matrix with real Chinese dataset. (30%)\n",
    "\n",
    "* 這邊的步驟為引入已經斷詞完成的欄位 ['segmented_lyrics']\n",
    "* 計算 tf-idf\n",
    "* 將字詞提請出來\n",
    "* 繪製成 tf-idf 表格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8433bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df_clean['segmented_lyrics'])\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "191a7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4425, 41111)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b155ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一一</th>\n",
       "      <th>一丁</th>\n",
       "      <th>一三三兩兩三三</th>\n",
       "      <th>一下</th>\n",
       "      <th>一下來</th>\n",
       "      <th>一下子</th>\n",
       "      <th>一不可</th>\n",
       "      <th>一世</th>\n",
       "      <th>一世富一世</th>\n",
       "      <th>一世界</th>\n",
       "      <th>...</th>\n",
       "      <th>龔德中</th>\n",
       "      <th>龔敬文</th>\n",
       "      <th>龔曉然</th>\n",
       "      <th>龔淑均</th>\n",
       "      <th>龔言脩</th>\n",
       "      <th>龔高飛</th>\n",
       "      <th>龜殼</th>\n",
       "      <th>龜毛</th>\n",
       "      <th>龜派</th>\n",
       "      <th>龜蛋</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4425 rows × 41111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       一一   一丁  一三三兩兩三三   一下  一下來  一下子  一不可   一世  一世富一世  一世界  ...  龔德中  龔敬文  \\\n",
       "0     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "1     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "2     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "3     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "...   ...  ...      ...  ...  ...  ...  ...  ...    ...  ...  ...  ...  ...   \n",
       "4420  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4421  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4422  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4423  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4424  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "      龔曉然  龔淑均  龔言脩  龔高飛   龜殼   龜毛   龜派   龜蛋  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "4420  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4421  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4422  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4423  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4424  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[4425 rows x 41111 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(vectors.toarray(),columns=feature_names)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c2819",
   "metadata": {},
   "source": [
    "## (2) Use the Chinese dataset to create your own embeddings and train a classification model with a neural network model. (30%)\n",
    "\n",
    "* 首先，先製作自己的 embeddings\n",
    "* 建構模型，並將 embeddings 放在第一層，進行模型訓練\n",
    "* 模型輸出維度為 10\n",
    "* 模型評估方式為 cross entropy 之 loss function 與 accuracy\n",
    "* 使用 cross entropy 是為了看預測與實際分佈之間的「距離」相差多少，即「損失了多少」（損失越小越好）\n",
    "* 使用 accuracy 而非 f-score 是因為想要純粹看模型「完全正確」，即 TP 與 TN 之判斷成果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8db73",
   "metadata": {},
   "source": [
    "#### embeddings 製作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "036a86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df_clean['segmented_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7addb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df_clean['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98509352",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_strings = ' '.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c305019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = set(docs_strings.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30068d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ee1b24e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35670, 5993, 24738, 5993, 28460, 34985, 34640, 3342, 4714, 6283, 4825, 33649, 15538, 34286, 38977, 12125, 18264, 4199, 37715, 31819, 3342, 11624, 33383, 33880, 29539, 33148, 37227, 33880, 15967, 7291, 34286, 17292, 33210, 7268, 23042, 33880, 14382, 39966, 42171, 18264, 16333, 27724, 38230, 17292, 34618, 22491, 17820, 17545, 12343, 34286, 18005, 12125, 3342, 4714, 7285, 19298, 36605, 15394, 34286, 29057, 9893, 34286, 37715, 25883, 33975, 18787, 17513, 17806, 17388, 1544, 19924, 12125, 39251, 34640, 33880, 19665, 38006, 27322, 43454, 1829, 15538, 31459, 17292, 4713, 12343, 34286, 32185, 24889, 17806, 33880, 13604, 32051, 20913, 18264, 18108, 33880, 17050, 10963, 29627, 34286, 9662, 17806, 33880, 25032, 41917, 29627, 34286, 8086, 43454, 1829, 7268, 23042, 42171, 11624, 42171, 11624, 33880, 12343, 7166, 34286, 31819, 5896, 33210, 39002, 15967, 38222, 30207, 30704, 38064, 37715, 8006, 37722, 38222, 30207, 33210, 39002, 22836, 20313, 17292, 34543, 33495, 28460, 34985, 34640, 3342, 4714, 6283, 4825, 33649, 15538, 34286, 38977, 12125, 18264, 4199, 37715, 31819, 3342, 11624, 33383, 33880, 29539, 33148, 37227, 33880, 15967, 7291, 34286, 17292, 33210, 7268, 23042, 33880, 14382, 39966, 42171, 18264, 16333, 27724, 38230, 17292, 34618, 22491, 17820, 17545, 12343, 34286, 18005, 12125, 3342, 4714, 7285, 19298, 36605, 15394, 34286, 23042, 39085, 9893, 34286, 37715, 25883, 33975, 18787, 17513, 17806, 17388, 1544, 19924, 12125, 39251, 34640, 33880, 19665, 38006, 27322, 43454, 1829, 15538, 31459, 17292, 4713, 12343, 34286, 32185, 24889, 17806, 33880, 13604, 32051, 20913, 18264, 18108, 33880, 17050, 10963, 29627, 34286, 9662, 17806, 33880, 25032, 41917, 29627, 34286, 8086, 43454, 1829, 7268, 23042, 42171, 11624, 42171, 11624, 33880, 12343, 7166, 34286, 31819, 5896, 33210, 39002, 15967, 38222, 30207, 30704, 38064, 37715, 8006, 37722, 38222, 30207, 33210, 39002, 22836, 20313, 17292, 34543, 33495, 43454, 1829, 15538, 31459, 17292, 4713, 12343, 34286, 32185, 24889, 17806, 33880, 13604, 32051, 20913, 18264, 18108, 33880, 17050, 10963, 29627, 34286, 9662, 17806, 33880, 25032, 41917, 29627, 34286, 8086, 43454, 1829, 7268, 23042, 42171, 11624, 42171, 11624, 33880, 12343, 7166, 34286, 31819, 5896, 33210, 39002, 15967, 38222, 30207, 30704, 38064, 37715, 8006, 37722, 38222, 30207, 33210, 39002, 22836, 20313, 17292, 34543, 33495], [35670, 16777, 24738, 5591, 19924, 17806, 12125, 6842, 17843, 39081, 34286, 433, 40589, 33880, 23644, 19298, 17292, 6842, 10963, 34868, 16188, 21837, 34445, 18581, 26091, 33210, 22739, 33880, 35855, 19924, 17806, 12125, 30447, 18574, 22333, 34286, 12722, 40589, 35958, 15078, 3973, 34286, 4747, 27599, 19453, 22800, 18936, 34402, 41941, 34640, 31169, 20001, 20001, 31062, 10515, 5099, 17292, 36241, 28692, 42171, 31352, 17976, 4701, 39715, 27987, 12339, 34286, 22221, 6603, 20319, 24565, 22333, 3947, 20001, 20001, 31062, 19987, 38218, 25996, 18192, 36093, 12609, 41799, 17292, 26720, 156, 15730, 26091, 34258, 33880, 28668, 28711, 7045, 19924, 17806, 12125, 30447, 18574, 22333, 34286, 12722, 40589, 35958, 15078, 3973, 34286, 4747, 27599, 19453, 22800, 18936, 34402, 41941, 34640, 31169, 20001, 20001, 31062, 10515, 5099, 17292, 36241, 28692, 42171, 31352, 17976, 4701, 39715, 27987, 12339, 34286, 22221, 6603, 20319, 24565, 22333, 3947, 20001, 20001, 31062, 19987, 38218, 25996, 18192, 36093, 12609, 41799, 17292, 26720, 156, 15730, 26091, 34258, 33880, 28668, 28711, 7045, 20001, 20001, 31062, 10515, 5099, 17292, 36241, 28692, 42171, 31352, 17976, 4701, 39715, 27987, 12339, 34286, 22221, 6603, 20319, 24565, 22333, 3947, 20001, 20001, 31062, 19987, 38218, 25996, 18192, 36093, 12609, 41799, 17292, 26720, 156, 15730, 26091, 34258, 33880, 28668, 28711, 7045], [35670, 39609, 24738, 39609, 17806, 28866, 34640, 14088, 27680, 36605, 26474, 15604, 19924, 19453, 13805, 16305, 22700, 11145, 17806, 10146, 34640, 14088, 27680, 34286, 26802, 22064, 38977, 19453, 7407, 37715, 26722, 34286, 38977, 15967, 9805, 42171, 18264, 16333, 15161, 33880, 34356, 21477, 12125, 39002, 1513, 19924, 17806, 24811, 6481, 34286, 16305, 33020, 15967, 19924, 17806, 15968, 29627, 34286, 38977, 29921, 34286, 16305, 33020, 15967, 14006, 19453, 22800, 37715, 22534, 34286, 16305, 33020, 15967, 16313, 34286, 29921, 17806, 26055, 16305, 17292, 34286, 16305, 33020, 15967, 14382, 15171, 15967, 9805, 6666, 17806, 28866, 34640, 14088, 27680, 36605, 26474, 15604, 19924, 19453, 13805, 16305, 22700, 11145, 17806, 10146, 34640, 14088, 27680, 34286, 26802, 22064, 38977, 19453, 7407, 37715, 26722, 34286, 38977, 15967, 9805, 42171, 18264, 16333, 15161, 33880, 34356, 21477, 12125, 39002, 1513, 19924, 17806, 24811, 6481, 34286, 16305, 33020, 15967, 19924, 17806, 15968, 29627, 34286, 38977, 29921, 34286, 16305, 33020, 15967, 14006, 19453, 22800, 37715, 22534, 34286, 16305, 33020, 15967, 16313, 34286, 29921, 17806, 26055, 16305, 17292, 34286, 16305, 33020, 15967, 14382, 15171, 15967, 9805, 6666, 34286, 16305, 33020, 15967, 19924, 17806, 15968, 29627, 34286, 38977, 29921, 34286, 16305, 33020, 15967, 14006, 19453, 22800, 37715, 22534, 34286, 16305, 33020, 15967, 16313, 34286, 29921, 17806, 26055, 16305, 17292, 34286, 16305, 33020, 15967, 14382, 15171, 15967, 9805, 6666]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "vocab_size = len(vocabs)\n",
    "hashed_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(hashed_docs[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86500a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(d.split()) for d in docs])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cbb8a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35670  5993 24738 ...     0     0     0]\n",
      " [35670 16777 24738 ...     0     0     0]\n",
      " [35670 39609 24738 ...     0     0     0]\n",
      " [35670 25827 24738 ...     0     0     0]\n",
      " [35670 16777 24738 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils import pad_sequences\n",
    "\n",
    "padded_docs = pad_sequences(hashed_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914f1fb",
   "metadata": {},
   "source": [
    "#### 模型訓練（包含模型搭建、加入前面訓練的 embeddings、列印 model summary 與 model evaluation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce83265c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 948, 10)           436580    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9480)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 9481      \n",
      "=================================================================\n",
      "Total params: 446,061\n",
      "Trainable params: 446,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 99.977404\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_docs, np.array(labels), epochs=50, verbose=0)\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# model evaluation\n",
    "loss, accuracy = model.evaluate(padded_docs, np.array(labels), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %d' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11349685",
   "metadata": {},
   "source": [
    "## 第二題 Reflection\n",
    "\n",
    "單純用 embeddings 訓練的 accuracy 竟然能達到 99.98！推測是因為資料本身詞彙量夠多，且分類之標籤（positive or negative sentiment）和歌詞本身息息相關，故能夠有如此好的效果。另外，其實我一開始用的是前幾週 yahoo 影評的資料，結果訓練出來結果非常差，accuracy 只有十幾，loss 是負數，所以我才猜測單獨使用 embeddings 的模型表現和文本本身的特性非常相關。因為影評普遍非常短、包含大量平常認為的 stop words 以及標點符號、表情符號等等，不利 word embeddings 的分類任務。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0dc24",
   "metadata": {},
   "source": [
    "## (3) Train the classification model with training set and test set. (20%)\n",
    "\n",
    "* 首先，因為歌詞資料爬取時是照著注音符號之順序，故在分 training set 與 test set 之前先進行 shuffle\n",
    "* 分為 training set 與 test set\n",
    "* 建構模型並放入前面自製的 embeddings 做分類任務\n",
    "* 評估方式比照前一題之模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d6fdf",
   "metadata": {},
   "source": [
    "#### shuffle data 並區分 training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffae0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shuffled_feature, shuffled_label = shuffle(padded_docs, labels, random_state = 888)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(shuffled_feature, shuffled_label, test_size = 0.2, random_state = 413)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab120aef",
   "metadata": {},
   "source": [
    "#### 模型訓練（包含模型搭建、加入前面訓練的 embeddings、列印 model summary 與 model evaluation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "538b8645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 948, 10)           436580    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9480)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9481      \n",
      "=================================================================\n",
      "Total params: 446,061\n",
      "Trainable params: 446,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 83.276838\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "\n",
    "model_new = Sequential()\n",
    "model_new.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "model_new.add(Flatten())\n",
    "model_new.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "\n",
    "model_new.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_new.fit(feature_train, np.array(label_train), epochs = 50, verbose = 0)\n",
    "\n",
    "\n",
    "# summarize the model\n",
    "\n",
    "print(model_new.summary())\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "loss_new, accuracy_new = model_new.evaluate(feature_test, np.array(label_test), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy_new*100))\n",
    "print('Loss: %d' % loss_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96df40a",
   "metadata": {},
   "source": [
    "## 第三題 Reflection\n",
    "\n",
    "訓練完後會發現雖然模型的 accuracy 還算高，但對比前一題沒有區分訓練與測試集的模型來說低了非常多。我猜是因為如果沒有區分的話訓練的資料也會等於要預測的資料，故模型很有可能會 overfitting（有點像是拿著答案找答案的感覺）。所以在做模型訓練時區分訓練集以及測試集是非常重要的，比較能看到模型的真正表現以及要改進的地方。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb743ef",
   "metadata": {},
   "source": [
    "## (4) Find an English dataset and train a model with pre-trained GloVe embeddings. Don't update the weight during training. And split the dataset into training set and test set before training a model. (20%)\n",
    "\n",
    "* 首先，讀入英文資料\n",
    "* 此資料由 Kaggle 取得，為奈及利亞工作招聘之廣告，其中包含真實與欺騙的廣告內容\n",
    "* 廣告為真實之 label 為 0，反之為 1，故此次模型分類目的為判斷廣告知真假\n",
    "* 進行斷詞\n",
    "* 接著下載 GloVe 預訓練好的 embeddings (6B, 100 dim)\n",
    "* 製作 embeddings matrix 和權重\n",
    "* 建構模型，於第一層 embeddings 放入 GloVe 資料，將 trainable 參數設置為 False，避免每次模型自行更新權重\n",
    "* shuffle 資料並區分 training and test set\n",
    "* 評估模型方式照前面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266609c",
   "metadata": {},
   "source": [
    "#### 讀入英文資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28a5db35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_desc</th>\n",
       "      <th>job_desc</th>\n",
       "      <th>job_requirement</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>department</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accountant</td>\n",
       "      <td>Equity Model Limited</td>\n",
       "      <td>Accounting, Auditing &amp; Finance</td>\n",
       "      <td>Compiling, analyzing, and reporting financial ...</td>\n",
       "      <td>This position is open preferably to a male can...</td>\n",
       "      <td>75,000 - 150,000</td>\n",
       "      <td>Abuja</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Law &amp; Compliance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Content Writer</td>\n",
       "      <td>CLINTON FUND (CF)</td>\n",
       "      <td>Management &amp; Business Development</td>\n",
       "      <td>Creating, improving and maintaining content to...</td>\n",
       "      <td>Bachelor's degree in Journalism, English, Com...</td>\n",
       "      <td>60,000 - 100,000</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Content Writing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accountant</td>\n",
       "      <td>Schleez Nigeria Limited</td>\n",
       "      <td>Accounting, Auditing &amp; Finance</td>\n",
       "      <td>Managing financial transactions, preparing fin...</td>\n",
       "      <td>Minimum of Bachelor's degree in Accounting or ...</td>\n",
       "      <td>Negotiable</td>\n",
       "      <td>First Floor, Left Wing, No. 49, Olowu Street, ...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>Bons Industries Limited</td>\n",
       "      <td>Marketing &amp; Communications</td>\n",
       "      <td>Understanding of the sales process and dynamics.\"</td>\n",
       "      <td>Minimum academic qualification of BSC/HND Degr...</td>\n",
       "      <td>75,000 - 150,000</td>\n",
       "      <td>Enugu</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Manufacturing &amp; Warehousing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bulk/Partnership Marketing Officer</td>\n",
       "      <td>TAMAK LOGISTICS</td>\n",
       "      <td>Marketing &amp; Communications</td>\n",
       "      <td>Establish relationships with major businesses ...</td>\n",
       "      <td>Be smart &amp; resourceful.,Great knowledge of how...</td>\n",
       "      <td>Less than 75,000</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Shipping &amp; Logistics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            job_title             company_name  \\\n",
       "0                          Accountant     Equity Model Limited   \n",
       "1                      Content Writer        CLINTON FUND (CF)   \n",
       "2                          Accountant  Schleez Nigeria Limited   \n",
       "3                     Sales Executive  Bons Industries Limited   \n",
       "4  Bulk/Partnership Marketing Officer          TAMAK LOGISTICS   \n",
       "\n",
       "                        company_desc  \\\n",
       "0     Accounting, Auditing & Finance   \n",
       "1  Management & Business Development   \n",
       "2     Accounting, Auditing & Finance   \n",
       "3         Marketing & Communications   \n",
       "4         Marketing & Communications   \n",
       "\n",
       "                                            job_desc  \\\n",
       "0  Compiling, analyzing, and reporting financial ...   \n",
       "1  Creating, improving and maintaining content to...   \n",
       "2  Managing financial transactions, preparing fin...   \n",
       "3  Understanding of the sales process and dynamics.\"   \n",
       "4  Establish relationships with major businesses ...   \n",
       "\n",
       "                                     job_requirement            salary  \\\n",
       "0  This position is open preferably to a male can...  75,000 - 150,000   \n",
       "1   Bachelor's degree in Journalism, English, Com...  60,000 - 100,000   \n",
       "2  Minimum of Bachelor's degree in Accounting or ...        Negotiable   \n",
       "3  Minimum academic qualification of BSC/HND Degr...  75,000 - 150,000   \n",
       "4  Be smart & resourceful.,Great knowledge of how...  Less than 75,000   \n",
       "\n",
       "                                            location employment_type  \\\n",
       "0                                              Abuja       Full Time   \n",
       "1                                              Lagos       Full Time   \n",
       "2  First Floor, Left Wing, No. 49, Olowu Street, ...       Full-time   \n",
       "3                                              Enugu       Full Time   \n",
       "4                                              Lagos       Full Time   \n",
       "\n",
       "                    department  label  \n",
       "0             Law & Compliance      0  \n",
       "1              Content Writing      1  \n",
       "2                   Accounting      1  \n",
       "3  Manufacturing & Warehousing      0  \n",
       "4         Shipping & Logistics      0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_eng = pd.read_csv('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/job_posting.csv', sep=',')\n",
    "df_eng.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "117fa457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Compiling, analyzing, and reporting financial data.',\n",
       " 'Creating, improving and maintaining content to achieve our business goals. Your duties will also include sharing content to raise brand awareness and monitoring web traffic and metrics to identify best practices. ',\n",
       " 'Managing financial transactions, preparing financial reports, and providing financial advice to clients.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_desc = list(df_eng['job_desc'])\n",
    "job_desc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72f4f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_eng = list(df_eng['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a616665",
   "metadata": {},
   "source": [
    "#### 英文斷詞、製作 one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "351962a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329, 208, 1, 41, 9, 116], [149, 330, 1, 35, 75, 2, 150, 13, 14, 98, 331, 332, 76, 333, 334, 335, 75, 2, 209, 336, 337, 1, 210, 338, 339, 1, 340, 2, 62, 341, 211], [36, 9, 63, 64, 9, 46, 1, 77, 9, 151, 2, 6], [342, 4, 3, 24, 58, 1, 343], [212, 17, 5, 344, 345, 346, 18, 3, 213]]\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(job_desc)\n",
    "\n",
    "vocab_size_eng = len(t.word_index) + 1\n",
    "hashed_sents = t.texts_to_sequences(job_desc)\n",
    "print(hashed_sents[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16b98da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "max_length_eng = max([len(s.split()) for s in job_desc])\n",
    "print(max_length_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e47be8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329 208   1  41   9 116   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [149 330   1  35  75   2 150  13  14  98 331 332  76 333 334 335  75   2\n",
      "  209 336 337   1 210 338 339   1 340   2  62 341 211   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 36   9  63  64   9  46   1  77   9 151   2   6   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [342   4   3  24  58   1 343   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [212  17   5 344 345 346  18   3 213   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length\n",
    "padded_sents = pad_sequences(hashed_sents, maxlen=max_length_eng, padding='post')\n",
    "print(padded_sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6ede2",
   "metadata": {},
   "source": [
    "#### 引入下載好的 GloVe pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51618381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/glove/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78102f97",
   "metadata": {},
   "source": [
    "#### 製作 weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29b882c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "\n",
    "embedding_matrix = zeros((vocab_size_eng, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbd631",
   "metadata": {},
   "source": [
    "#### shuffle data 並區分 training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8e94634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shuffled_feature_eng, shuffled_label_eng = shuffle(padded_sents, labels_eng, random_state = 888)\n",
    "feature_train_eng, feature_test_eng, label_train_eng, label_test_eng = train_test_split(shuffled_feature_eng, shuffled_label_eng, test_size = 0.2, random_state = 413)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3252db",
   "metadata": {},
   "source": [
    "#### 模型訓練（包含模型搭建、加入前面訓練的 embeddings、列印 model summary 與 model evaluation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16cbfbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 115, 100)          83800     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 11500)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11501     \n",
      "=================================================================\n",
      "Total params: 95,301\n",
      "Trainable params: 11,501\n",
      "Non-trainable params: 83,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 85.365856\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "model_eng = Sequential()\n",
    "e = Embedding(vocab_size_eng, 100, weights=[embedding_matrix], input_length= max_length_eng, trainable=False)\n",
    "model_eng.add(e)\n",
    "model_eng.add(Flatten())\n",
    "model_eng.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model_eng.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_eng.fit(feature_train_eng, np.array(label_train_eng), epochs = 50, verbose = 0)\n",
    "\n",
    "# summarize the model\n",
    "print(model_eng.summary())\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "loss_eng, accuracy_eng = model_eng.evaluate(feature_test_eng, np.array(label_test_eng), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy_eng*100))\n",
    "print('Loss: %d' % loss_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b156f69",
   "metadata": {},
   "source": [
    "## 第四題 Reflection\n",
    "\n",
    "英文的資料雖然沒有用自製的 word embeddings，但是顯然 GloVe pre-trained embeddings 表現良好，accuracy 達到 85.4，loss 為零，所以證明可以用 pre-trained embeddings 達到想要的預測目標。且這個的好處是不用每次都要自己訓練、擔心有 overfitting 的問題，可以應用於不同內容的文本都有著不錯的表現。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6565d9c",
   "metadata": {},
   "source": [
    "## BONUS-1\n",
    "### play around with the embedding visualization\n",
    "\n",
    "* 這邊要可視化的是中文、有分 training and test set 的模型\n",
    "* 取出第一層 embeddings 的資料（權重）\n",
    "* 列出所有 words\n",
    "* 分別存成 Embedding Projector 可以讀取之檔案\n",
    "* 可視化 (成果為附件之影片檔)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd138e4",
   "metadata": {},
   "source": [
    "* 取出 embedding 層每個 checkpoint 的 權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f359b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43658, 10)\n"
     ]
    }
   ],
   "source": [
    "emb_layer = model_new.layers[0]\n",
    "weights = emb_layer.get_weights()[0]\n",
    "print(weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0ef87",
   "metadata": {},
   "source": [
    "* 列出 word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97962ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bc6b7",
   "metadata": {},
   "source": [
    "#### 分別存成 tsv 檔，以便上傳至 Embedding Projector 來進行可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c52a80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "weight_vec = io.open('weight_vec.tsv', 'w', encoding='utf-8')\n",
    "word_arr = io.open('words.tsv', 'w', encoding='utf-8')\n",
    "for word in words:\n",
    "    word_vec = weights[i]\n",
    "    word_arr.write(word + \"\\n\")\n",
    "    weight_vec.write('\\t'.join([str(x) for x in word_vec]) + '\\n')\n",
    "    i=i+1\n",
    "\n",
    "weight_vec.close()\n",
    "word_arr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd50ea",
   "metadata": {},
   "source": [
    "## BONUS-2\n",
    "### play around with the GPU server and train with a larger GloVe embeddings\n",
    "\n",
    "* 這邊下載了最大的 GloVe embeddings (840 B, 300 dim)\n",
    "* 比照前述方式進行模型訓練\n",
    "* p.s. 原本要試試看 Word2Vec 之類的，結果 gensim pacakage 和 numpy 版本衝突，升級 numpy 之後換 numpy 和 tensorflow 衝突......只好放棄><"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfe96a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196007 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_index_lar = dict()\n",
    "\n",
    "with open('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/glove.840B.300d.txt', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    \n",
    "    values = line.strip().split(' ')\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_lar[word] = coefs\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index_lar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3dbafd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "\n",
    "embedding_matrix_lar = zeros((vocab_size_eng, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector_lar = embeddings_index_lar.get(word)\n",
    "    if embedding_vector_lar is not None:\n",
    "        embedding_matrix_lar[i] = embedding_vector_lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bbe1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 115, 300)          251400    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 34500)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 34501     \n",
      "=================================================================\n",
      "Total params: 285,901\n",
      "Trainable params: 34,501\n",
      "Non-trainable params: 251,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 80.487806\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "model_eng_lar = Sequential()\n",
    "e_lar = Embedding(vocab_size_eng, 300, weights=[embedding_matrix_lar], input_length= max_length_eng, trainable=False)\n",
    "model_eng_lar.add(e_lar)\n",
    "model_eng_lar.add(Flatten())\n",
    "model_eng_lar.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model_eng_lar.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_eng_lar.fit(feature_train_eng, np.array(label_train_eng), epochs = 50, verbose = 0)\n",
    "\n",
    "# summarize the model\n",
    "print(model_eng_lar.summary())\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "loss_eng_lar, accuracy_eng_lar = model_eng_lar.evaluate(feature_test_eng, np.array(label_test_eng), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy_eng_lar*100))\n",
    "print('Loss: %d' % loss_eng_lar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d3149",
   "metadata": {},
   "source": [
    "## BONUS-2 Reflection\n",
    "\n",
    "費盡千辛萬可下載最大的 GloVe embeddings（檔案非常大），結果表現沒有 6 B 100 dim 的 embeddings 好 ><。我猜是因為資料太多且維度也過多，導致模型在訓練時有很多雜訊影響其判斷，才會降低了一些效果。因此，我建議在本身 dataset 沒有很大的時候用普通大小的 word embeddings 就足夠了。另外，補充個小資訊，'glove.840B.300d.txt' 裡面有不正常的符號，所以在轉換成 32 float 的時候會出現報錯，解決方式就是在 readlines 的時候加入 strip( ) 把沒用的符號去除掉才能夠正常運行。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
