{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76456f1",
   "metadata": {},
   "source": [
    "Computational Linguistics &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; <br>Biao Yun\n",
    "\n",
    "<center> \n",
    "\n",
    "# Word Embeddings Assignment\n",
    "### 2023.03.20\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea2718",
   "metadata": {},
   "source": [
    "#### å¼•å…¥æ‰€éœ€ packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca7eef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ckiptagger import data_utils\n",
    "from ckiptagger import WS\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2970ad",
   "metadata": {},
   "source": [
    "#### è¼‰å…¥ CKIP æ–·è©ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b30098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/biaoyun/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/Users/biaoyun/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ws = WS(\"/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddae13",
   "metadata": {},
   "source": [
    "## è®€å…¥ä¸­æ–‡æ–‡ä»¶\n",
    "\n",
    "* æ­¤æª”æ¡ˆç‚ºè¯èªç”·æ­Œæ‰‹ 2013-2023 ä¹‹é–“ç™¼å¸ƒä¹‹æ­Œæ›²æ­Œè©ï¼ŒåŒ…å«æ­Œæ‰‹ã€æ­Œåã€æ­Œè©ã€ç¶²å€èˆ‡æ—¥æœŸç­‰æ¬„ä½ï¼Œç”±é­”é¡æ­Œè©ç¶²å–å¾—\n",
    "* ç”±æ–¼ CKIP æ–·è©ç³»çµ±ä¸å ªè² è·ï¼Œå¾åŸæª”æ¡ˆå…«è¬å¤šé¦–æ­Œä¸­éš¨æ©Ÿå–å‡º5000é¦–æ­Œä½œç‚ºæ­¤æ¬¡æ¨¡å‹è¨“ç·´è³‡æ–™\n",
    "* æ¨¡å‹åˆ†é¡ç›®çš„ç‚ºåˆ¤æ–·æ˜¯å¦ç‚º positive or negative çš„æ­Œè©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5558742f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/male_all_recent_10_years_song_lyrics_content_clean.csv', sep='\\t')\n",
    "df = df.iloc[29585:34585, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d648d223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singer</th>\n",
       "      <th>name</th>\n",
       "      <th>lyric</th>\n",
       "      <th>song_url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29585</th>\n",
       "      <td>é«˜å±±æ—</td>\n",
       "      <td>äº”æœˆèŠ±é–‹</td>\n",
       "      <td>ä½œè©ï¼šå­«æµ©, ä½œæ›²ï¼šå­«æµ©, çµ‚æ–¼ç›¼åˆ°äº†æ»¿åœ’æ˜¥è‰²éåœ°èŠ±é–‹, ä½ èªªé€™å€‹å­£ç¯€æœƒå›ä¾†, æ»¿å¤©é£›èˆçš„èŠ±...</td>\n",
       "      <td>twy131563x1x4.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29586</th>\n",
       "      <td>é«˜å±±æ—</td>\n",
       "      <td>å…„å¼Ÿä¿é‡</td>\n",
       "      <td>ä½œè©ï¼šå‘¨å…µ, ä½œæ›²ï¼šçŸ³ç„±, è®“æˆ‘é€™æ¯é…’ ç‚ºä½ é€è¡Œ, å¤ªå¤šçš„è©± éƒ½åœ¨æ¯è£¡, äººç”Ÿæ³¨å®šæœ‰çŸ­æš«åˆ†é›¢...</td>\n",
       "      <td>twy131563x1x3.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29587</th>\n",
       "      <td>é«˜å±±æ—</td>\n",
       "      <td>ä½ ä¸æ‡‚æ„›</td>\n",
       "      <td>ä½œè©ï¼šé˜¿åš, ä½œæ›²ï¼šé˜¿åš, æˆ‘å•äº†å¾ˆä¹…, æ˜¯ä»€éº¼åŸç”±, è®“æˆ‘å€‘èµ°ä¸åˆ°ç›¡é ­, æˆ‘ç­‰äº†å¾ˆä¹…, ä½ ...</td>\n",
       "      <td>twy131563x1x5.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29588</th>\n",
       "      <td>é«˜å±±æ—</td>\n",
       "      <td>ä¹˜é¢¨ç ´æµª</td>\n",
       "      <td>ä½œè©ï¼šå°ç™½, ä½œæ›²ï¼šå°ç™½, æšå¸†å•Ÿèˆª æµ·å²¸å¤©éš›, ä»°æœ›è’¼ç©¹ é³¥ç°å¤§åœ°, ç”Ÿå‘½å¦‚æ­Œ ç‘°éº—å¦‚è™¹,...</td>\n",
       "      <td>twy131563x1x11.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29589</th>\n",
       "      <td>é«˜å±±æ—</td>\n",
       "      <td>æ„›ä½ å¿…é ˆçš„</td>\n",
       "      <td>ä½œè©ï¼šå‘¨å…µ, ä½œæ›²ï¼šçŸ³ç„±, å›æ†¶æ¼¸æ¼¸é–‹å§‹é£„è½, ç‰½éä½ çš„æ‰‹é‚„è¨˜å¾—, ç¾å¤¢ç°¡å–®è¤‡è£½äº†å¿«æ¨‚, å­˜...</td>\n",
       "      <td>twy131563x1x2.htm</td>\n",
       "      <td>2014-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      singer   name                                              lyric  \\\n",
       "29585    é«˜å±±æ—   äº”æœˆèŠ±é–‹  ä½œè©ï¼šå­«æµ©, ä½œæ›²ï¼šå­«æµ©, çµ‚æ–¼ç›¼åˆ°äº†æ»¿åœ’æ˜¥è‰²éåœ°èŠ±é–‹, ä½ èªªé€™å€‹å­£ç¯€æœƒå›ä¾†, æ»¿å¤©é£›èˆçš„èŠ±...   \n",
       "29586    é«˜å±±æ—   å…„å¼Ÿä¿é‡  ä½œè©ï¼šå‘¨å…µ, ä½œæ›²ï¼šçŸ³ç„±, è®“æˆ‘é€™æ¯é…’ ç‚ºä½ é€è¡Œ, å¤ªå¤šçš„è©± éƒ½åœ¨æ¯è£¡, äººç”Ÿæ³¨å®šæœ‰çŸ­æš«åˆ†é›¢...   \n",
       "29587    é«˜å±±æ—   ä½ ä¸æ‡‚æ„›  ä½œè©ï¼šé˜¿åš, ä½œæ›²ï¼šé˜¿åš, æˆ‘å•äº†å¾ˆä¹…, æ˜¯ä»€éº¼åŸç”±, è®“æˆ‘å€‘èµ°ä¸åˆ°ç›¡é ­, æˆ‘ç­‰äº†å¾ˆä¹…, ä½ ...   \n",
       "29588    é«˜å±±æ—   ä¹˜é¢¨ç ´æµª  ä½œè©ï¼šå°ç™½, ä½œæ›²ï¼šå°ç™½, æšå¸†å•Ÿèˆª æµ·å²¸å¤©éš›, ä»°æœ›è’¼ç©¹ é³¥ç°å¤§åœ°, ç”Ÿå‘½å¦‚æ­Œ ç‘°éº—å¦‚è™¹,...   \n",
       "29589    é«˜å±±æ—  æ„›ä½ å¿…é ˆçš„  ä½œè©ï¼šå‘¨å…µ, ä½œæ›²ï¼šçŸ³ç„±, å›æ†¶æ¼¸æ¼¸é–‹å§‹é£„è½, ç‰½éä½ çš„æ‰‹é‚„è¨˜å¾—, ç¾å¤¢ç°¡å–®è¤‡è£½äº†å¿«æ¨‚, å­˜...   \n",
       "\n",
       "                 song_url     date  \n",
       "29585   twy131563x1x4.htm  2014-05  \n",
       "29586   twy131563x1x3.htm  2014-05  \n",
       "29587   twy131563x1x5.htm  2014-05  \n",
       "29588  twy131563x1x11.htm  2014-05  \n",
       "29589   twy131563x1x2.htm  2014-05  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6277c11",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "* é¦–å…ˆå°‡æ­Œè©æ¬„ä½çš„ã€Œä¸­æ–‡ã€æå–å‡ºä¾†ï¼ˆå³åˆªæ‰è‹±æ–‡æˆ–æ˜¯å…¶ä»–èªè¨€ä¹‹æ­Œè©ï¼‰\n",
    "* æ¥è‘—å°‡æ­Œè©ç”¨ CKIP é€²è¡Œæ–·è©ï¼ˆé€™é‚ŠçœŸçš„è·‘è¶…ï½ï½ä¹…ï¼Œæˆ‘åŸæœ¬æƒ³ç”¨ä¸€è¬é¦–ï¼Œçµæœ kernel ç•¶æ‰å…©æ¬¡ğŸ˜¨åªå¥½æ›æˆäº”åƒé¦–ï¼‰\n",
    "* å†ä¾†å¼•å…¥ LIWC ç¹é«”ä¸­æ–‡æƒ…ç·’è©å…¸ï¼Œå°æ­Œè©é€²è¡Œç°¡å–®çš„æ­£è² ç›¸æƒ…ç·’åˆ¤åˆ¥ï¼Œç”¢ç”Ÿ sentiment æ¬„ä½ï¼Œå³ä¹‹å¾Œåˆ†é¡æ¨¡å‹çš„ label\n",
    "* æ­£è² å‘æƒ…ç·’çµ¦å®šæ–¹å¼ç‚ºè¨ˆç®—æ¯é¦–æ­Œæ‰€å«æœ‰ä¹‹æ­£å‘è©èˆ‡è² å‘è©ï¼Œç›¸æ¸›å¾Œå¤§æ–¼ 0 ç‚ºæ­£å‘ï¼Œå°æ–¼ 0 ç‚ºè² å‘ï¼Œç­‰æ–¼ 0 ç‚ºä¸­æ€§\n",
    "* ç‚ºæ¸›å°‘æ¨¡å‹é›œè¨Šèˆ‡è² æ“”ï¼Œå°‡æƒ…ç·’ç‚ºä¸­æ€§ä¹‹æ­Œè©åˆªé™¤\n",
    "* æ­£å‘ä¹‹æ­Œè©çµ¦å®š label ç‚º 0ï¼Œè² å‘å‰‡ç‚º 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c964c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–ä¸­æ–‡å­—ä¹‹ function\n",
    "\n",
    "def extract_chinese(text):\n",
    "    pattern = re.compile(\"[\\u4e00-\\u9fa5]\")\n",
    "    \n",
    "    return \"\".join(pattern.findall(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de94070",
   "metadata": {},
   "outputs": [],
   "source": [
    "ly_list = list(df['lyric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d7956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ly = []\n",
    "\n",
    "for i in range(len(ly_list)):\n",
    "    new_ly.append(extract_chinese(ly_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e95fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lyric_new'] = new_ly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a82805",
   "metadata": {},
   "source": [
    "#### æ–·è©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47ae218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [38:12<00:00,  2.18it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_list = []\n",
    "for text in tqdm(new_ly):\n",
    "    tokens = ws([text])\n",
    "    token_list.append(tokens)\n",
    "# for text in df[\"lyric\"]:\n",
    "#   tokens = ws([text])\n",
    "#   token_list.append(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9aef2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flatten_tokens = []\n",
    "for list_1 in token_list:\n",
    "  for sublist in list_1:\n",
    "    flatten_tokens.append(sublist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6a474",
   "metadata": {},
   "source": [
    "#### è¨ˆç®—æ­Œè©ä¹‹æƒ…ç·’å‘åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f28c2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment\n",
    "\n",
    "LIWC = pd.read_csv(\"/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/LIWC.csv\", sep=',')\n",
    "\n",
    "pos = [31]\n",
    "neg = [32]\n",
    "for num_31 in pos:\n",
    "    pos_df = LIWC[LIWC.isin([num_31]).any(1)].reset_index(drop=True)\n",
    "for num_32 in neg:\n",
    "    neg_df = LIWC[LIWC.isin([num_32]).any(1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff7a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "negemo_list = []\n",
    "\n",
    "for neg in neg_df['%']:\n",
    "    negemo_list.append(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab275a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posemo_list = []\n",
    "\n",
    "for pos in pos_df['%']:\n",
    "    posemo_list.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c98965c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "neutral = []\n",
    "for tokens in token_list:\n",
    "  pos_counter = 0\n",
    "  neg_counter = 0\n",
    "  neu = 0\n",
    "  for i in tokens[0]:\n",
    "    if i in posemo_list:\n",
    "      pos_counter = pos_counter+1\n",
    "  for i in tokens[0]:\n",
    "    if i in negemo_list:\n",
    "      neg_counter = neg_counter+1\n",
    "  if pos_counter == 0 and neg_counter == 0:\n",
    "    neu = 1\n",
    "  else:\n",
    "    new = 0\n",
    "  positive.append(pos_counter)\n",
    "  negative.append(neg_counter)\n",
    "  neutral.append(neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff57c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = []\n",
    "for tokens in token_list:\n",
    "  pos_count = 0\n",
    "  neg_count = 0\n",
    "  for i in tokens[0]:\n",
    "    if i in posemo_list:\n",
    "      pos_count = pos_count+1\n",
    "    if i in negemo_list:\n",
    "      neg_count = neg_count+1\n",
    "  if pos_count == 0 and neg_count == 0:\n",
    "    result = 2\n",
    "  elif pos_count-neg_count > 0:\n",
    "    result = 0\n",
    "  elif pos_count-neg_count < 0:\n",
    "    result = 1\n",
    "  emo.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f77a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56844e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_string = []\n",
    "\n",
    "for ft in flatten_tokens:\n",
    "    st = ' '.join(ft)\n",
    "    flatten_string.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4e4cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segmented_lyrics'] = flatten_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a60b0f",
   "metadata": {},
   "source": [
    "#### åˆªé™¤æƒ…ç·’ç‚ºä¸­æ€§ä¹‹æ­Œè©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de8e0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df.sentiment != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜æª”æ¡ˆï¼Œæ‰ä¸æœƒæ¯æ¬¡éƒ½è¦é‡æ–°æ–·è©\n",
    "\n",
    "df_clean.to_csv('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Week_5/df_clean_new.csv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda4b65",
   "metadata": {},
   "source": [
    "## (1) Create a tf-idf matrix with real Chinese dataset. (30%)\n",
    "\n",
    "* é€™é‚Šçš„æ­¥é©Ÿç‚ºå¼•å…¥å·²ç¶“æ–·è©å®Œæˆçš„æ¬„ä½ ['segmented_lyrics']\n",
    "* è¨ˆç®— tf-idf\n",
    "* å°‡å­—è©æè«‹å‡ºä¾†\n",
    "* ç¹ªè£½æˆ tf-idf è¡¨æ ¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8433bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df_clean['segmented_lyrics'])\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "191a7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4425, 41111)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b155ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ä¸€ä¸€</th>\n",
       "      <th>ä¸€ä¸</th>\n",
       "      <th>ä¸€ä¸‰ä¸‰å…©å…©ä¸‰ä¸‰</th>\n",
       "      <th>ä¸€ä¸‹</th>\n",
       "      <th>ä¸€ä¸‹ä¾†</th>\n",
       "      <th>ä¸€ä¸‹å­</th>\n",
       "      <th>ä¸€ä¸å¯</th>\n",
       "      <th>ä¸€ä¸–</th>\n",
       "      <th>ä¸€ä¸–å¯Œä¸€ä¸–</th>\n",
       "      <th>ä¸€ä¸–ç•Œ</th>\n",
       "      <th>...</th>\n",
       "      <th>é¾”å¾·ä¸­</th>\n",
       "      <th>é¾”æ•¬æ–‡</th>\n",
       "      <th>é¾”æ›‰ç„¶</th>\n",
       "      <th>é¾”æ·‘å‡</th>\n",
       "      <th>é¾”è¨€è„©</th>\n",
       "      <th>é¾”é«˜é£›</th>\n",
       "      <th>é¾œæ®¼</th>\n",
       "      <th>é¾œæ¯›</th>\n",
       "      <th>é¾œæ´¾</th>\n",
       "      <th>é¾œè›‹</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4425 rows Ã— 41111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ä¸€ä¸€   ä¸€ä¸  ä¸€ä¸‰ä¸‰å…©å…©ä¸‰ä¸‰   ä¸€ä¸‹  ä¸€ä¸‹ä¾†  ä¸€ä¸‹å­  ä¸€ä¸å¯   ä¸€ä¸–  ä¸€ä¸–å¯Œä¸€ä¸–  ä¸€ä¸–ç•Œ  ...  é¾”å¾·ä¸­  é¾”æ•¬æ–‡  \\\n",
       "0     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "1     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "2     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "3     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4     0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "...   ...  ...      ...  ...  ...  ...  ...  ...    ...  ...  ...  ...  ...   \n",
       "4420  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4421  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4422  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4423  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "4424  0.0  0.0      0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "      é¾”æ›‰ç„¶  é¾”æ·‘å‡  é¾”è¨€è„©  é¾”é«˜é£›   é¾œæ®¼   é¾œæ¯›   é¾œæ´¾   é¾œè›‹  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "4420  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4421  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4422  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4423  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4424  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[4425 rows x 41111 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(vectors.toarray(),columns=feature_names)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c2819",
   "metadata": {},
   "source": [
    "## (2) Use the Chinese dataset to create your own embeddings and train a classification model with a neural network model. (30%)\n",
    "\n",
    "* é¦–å…ˆï¼Œå…ˆè£½ä½œè‡ªå·±çš„ embeddings\n",
    "* å»ºæ§‹æ¨¡å‹ï¼Œä¸¦å°‡ embeddings æ”¾åœ¨ç¬¬ä¸€å±¤ï¼Œé€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "* æ¨¡å‹è¼¸å‡ºç¶­åº¦ç‚º 10\n",
    "* æ¨¡å‹è©•ä¼°æ–¹å¼ç‚º cross entropy ä¹‹ loss function èˆ‡ accuracy\n",
    "* ä½¿ç”¨ cross entropy æ˜¯ç‚ºäº†çœ‹é æ¸¬èˆ‡å¯¦éš›åˆ†ä½ˆä¹‹é–“çš„ã€Œè·é›¢ã€ç›¸å·®å¤šå°‘ï¼Œå³ã€Œæå¤±äº†å¤šå°‘ã€ï¼ˆæå¤±è¶Šå°è¶Šå¥½ï¼‰\n",
    "* ä½¿ç”¨ accuracy è€Œé f-score æ˜¯å› ç‚ºæƒ³è¦ç´”ç²¹çœ‹æ¨¡å‹ã€Œå®Œå…¨æ­£ç¢ºã€ï¼Œå³ TP èˆ‡ TN ä¹‹åˆ¤æ–·æˆæœ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8db73",
   "metadata": {},
   "source": [
    "#### embeddings è£½ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "036a86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df_clean['segmented_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7addb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df_clean['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98509352",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_strings = ' '.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c305019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = set(docs_strings.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30068d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ee1b24e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35670, 5993, 24738, 5993, 28460, 34985, 34640, 3342, 4714, 6283, 4825, 33649, 15538, 34286, 38977, 12125, 18264, 4199, 37715, 31819, 3342, 11624, 33383, 33880, 29539, 33148, 37227, 33880, 15967, 7291, 34286, 17292, 33210, 7268, 23042, 33880, 14382, 39966, 42171, 18264, 16333, 27724, 38230, 17292, 34618, 22491, 17820, 17545, 12343, 34286, 18005, 12125, 3342, 4714, 7285, 19298, 36605, 15394, 34286, 29057, 9893, 34286, 37715, 25883, 33975, 18787, 17513, 17806, 17388, 1544, 19924, 12125, 39251, 34640, 33880, 19665, 38006, 27322, 43454, 1829, 15538, 31459, 17292, 4713, 12343, 34286, 32185, 24889, 17806, 33880, 13604, 32051, 20913, 18264, 18108, 33880, 17050, 10963, 29627, 34286, 9662, 17806, 33880, 25032, 41917, 29627, 34286, 8086, 43454, 1829, 7268, 23042, 42171, 11624, 42171, 11624, 33880, 12343, 7166, 34286, 31819, 5896, 33210, 39002, 15967, 38222, 30207, 30704, 38064, 37715, 8006, 37722, 38222, 30207, 33210, 39002, 22836, 20313, 17292, 34543, 33495, 28460, 34985, 34640, 3342, 4714, 6283, 4825, 33649, 15538, 34286, 38977, 12125, 18264, 4199, 37715, 31819, 3342, 11624, 33383, 33880, 29539, 33148, 37227, 33880, 15967, 7291, 34286, 17292, 33210, 7268, 23042, 33880, 14382, 39966, 42171, 18264, 16333, 27724, 38230, 17292, 34618, 22491, 17820, 17545, 12343, 34286, 18005, 12125, 3342, 4714, 7285, 19298, 36605, 15394, 34286, 23042, 39085, 9893, 34286, 37715, 25883, 33975, 18787, 17513, 17806, 17388, 1544, 19924, 12125, 39251, 34640, 33880, 19665, 38006, 27322, 43454, 1829, 15538, 31459, 17292, 4713, 12343, 34286, 32185, 24889, 17806, 33880, 13604, 32051, 20913, 18264, 18108, 33880, 17050, 10963, 29627, 34286, 9662, 17806, 33880, 25032, 41917, 29627, 34286, 8086, 43454, 1829, 7268, 23042, 42171, 11624, 42171, 11624, 33880, 12343, 7166, 34286, 31819, 5896, 33210, 39002, 15967, 38222, 30207, 30704, 38064, 37715, 8006, 37722, 38222, 30207, 33210, 39002, 22836, 20313, 17292, 34543, 33495, 43454, 1829, 15538, 31459, 17292, 4713, 12343, 34286, 32185, 24889, 17806, 33880, 13604, 32051, 20913, 18264, 18108, 33880, 17050, 10963, 29627, 34286, 9662, 17806, 33880, 25032, 41917, 29627, 34286, 8086, 43454, 1829, 7268, 23042, 42171, 11624, 42171, 11624, 33880, 12343, 7166, 34286, 31819, 5896, 33210, 39002, 15967, 38222, 30207, 30704, 38064, 37715, 8006, 37722, 38222, 30207, 33210, 39002, 22836, 20313, 17292, 34543, 33495], [35670, 16777, 24738, 5591, 19924, 17806, 12125, 6842, 17843, 39081, 34286, 433, 40589, 33880, 23644, 19298, 17292, 6842, 10963, 34868, 16188, 21837, 34445, 18581, 26091, 33210, 22739, 33880, 35855, 19924, 17806, 12125, 30447, 18574, 22333, 34286, 12722, 40589, 35958, 15078, 3973, 34286, 4747, 27599, 19453, 22800, 18936, 34402, 41941, 34640, 31169, 20001, 20001, 31062, 10515, 5099, 17292, 36241, 28692, 42171, 31352, 17976, 4701, 39715, 27987, 12339, 34286, 22221, 6603, 20319, 24565, 22333, 3947, 20001, 20001, 31062, 19987, 38218, 25996, 18192, 36093, 12609, 41799, 17292, 26720, 156, 15730, 26091, 34258, 33880, 28668, 28711, 7045, 19924, 17806, 12125, 30447, 18574, 22333, 34286, 12722, 40589, 35958, 15078, 3973, 34286, 4747, 27599, 19453, 22800, 18936, 34402, 41941, 34640, 31169, 20001, 20001, 31062, 10515, 5099, 17292, 36241, 28692, 42171, 31352, 17976, 4701, 39715, 27987, 12339, 34286, 22221, 6603, 20319, 24565, 22333, 3947, 20001, 20001, 31062, 19987, 38218, 25996, 18192, 36093, 12609, 41799, 17292, 26720, 156, 15730, 26091, 34258, 33880, 28668, 28711, 7045, 20001, 20001, 31062, 10515, 5099, 17292, 36241, 28692, 42171, 31352, 17976, 4701, 39715, 27987, 12339, 34286, 22221, 6603, 20319, 24565, 22333, 3947, 20001, 20001, 31062, 19987, 38218, 25996, 18192, 36093, 12609, 41799, 17292, 26720, 156, 15730, 26091, 34258, 33880, 28668, 28711, 7045], [35670, 39609, 24738, 39609, 17806, 28866, 34640, 14088, 27680, 36605, 26474, 15604, 19924, 19453, 13805, 16305, 22700, 11145, 17806, 10146, 34640, 14088, 27680, 34286, 26802, 22064, 38977, 19453, 7407, 37715, 26722, 34286, 38977, 15967, 9805, 42171, 18264, 16333, 15161, 33880, 34356, 21477, 12125, 39002, 1513, 19924, 17806, 24811, 6481, 34286, 16305, 33020, 15967, 19924, 17806, 15968, 29627, 34286, 38977, 29921, 34286, 16305, 33020, 15967, 14006, 19453, 22800, 37715, 22534, 34286, 16305, 33020, 15967, 16313, 34286, 29921, 17806, 26055, 16305, 17292, 34286, 16305, 33020, 15967, 14382, 15171, 15967, 9805, 6666, 17806, 28866, 34640, 14088, 27680, 36605, 26474, 15604, 19924, 19453, 13805, 16305, 22700, 11145, 17806, 10146, 34640, 14088, 27680, 34286, 26802, 22064, 38977, 19453, 7407, 37715, 26722, 34286, 38977, 15967, 9805, 42171, 18264, 16333, 15161, 33880, 34356, 21477, 12125, 39002, 1513, 19924, 17806, 24811, 6481, 34286, 16305, 33020, 15967, 19924, 17806, 15968, 29627, 34286, 38977, 29921, 34286, 16305, 33020, 15967, 14006, 19453, 22800, 37715, 22534, 34286, 16305, 33020, 15967, 16313, 34286, 29921, 17806, 26055, 16305, 17292, 34286, 16305, 33020, 15967, 14382, 15171, 15967, 9805, 6666, 34286, 16305, 33020, 15967, 19924, 17806, 15968, 29627, 34286, 38977, 29921, 34286, 16305, 33020, 15967, 14006, 19453, 22800, 37715, 22534, 34286, 16305, 33020, 15967, 16313, 34286, 29921, 17806, 26055, 16305, 17292, 34286, 16305, 33020, 15967, 14382, 15171, 15967, 9805, 6666]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "vocab_size = len(vocabs)\n",
    "hashed_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(hashed_docs[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86500a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(d.split()) for d in docs])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cbb8a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35670  5993 24738 ...     0     0     0]\n",
      " [35670 16777 24738 ...     0     0     0]\n",
      " [35670 39609 24738 ...     0     0     0]\n",
      " [35670 25827 24738 ...     0     0     0]\n",
      " [35670 16777 24738 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils import pad_sequences\n",
    "\n",
    "padded_docs = pad_sequences(hashed_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914f1fb",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹è¨“ç·´ï¼ˆåŒ…å«æ¨¡å‹æ­å»ºã€åŠ å…¥å‰é¢è¨“ç·´çš„ embeddingsã€åˆ—å° model summary èˆ‡ model evaluationï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce83265c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 948, 10)           436580    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9480)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 9481      \n",
      "=================================================================\n",
      "Total params: 446,061\n",
      "Trainable params: 446,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 99.977404\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_docs, np.array(labels), epochs=50, verbose=0)\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# model evaluation\n",
    "loss, accuracy = model.evaluate(padded_docs, np.array(labels), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %d' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11349685",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒé¡Œ Reflection\n",
    "\n",
    "å–®ç´”ç”¨ embeddings è¨“ç·´çš„ accuracy ç«Ÿç„¶èƒ½é”åˆ° 99.98ï¼æ¨æ¸¬æ˜¯å› ç‚ºè³‡æ–™æœ¬èº«è©å½™é‡å¤ å¤šï¼Œä¸”åˆ†é¡ä¹‹æ¨™ç±¤ï¼ˆpositive or negative sentimentï¼‰å’Œæ­Œè©æœ¬èº«æ¯æ¯ç›¸é—œï¼Œæ•…èƒ½å¤ æœ‰å¦‚æ­¤å¥½çš„æ•ˆæœã€‚å¦å¤–ï¼Œå…¶å¯¦æˆ‘ä¸€é–‹å§‹ç”¨çš„æ˜¯å‰å¹¾é€± yahoo å½±è©•çš„è³‡æ–™ï¼Œçµæœè¨“ç·´å‡ºä¾†çµæœéå¸¸å·®ï¼Œaccuracy åªæœ‰åå¹¾ï¼Œloss æ˜¯è² æ•¸ï¼Œæ‰€ä»¥æˆ‘æ‰çŒœæ¸¬å–®ç¨ä½¿ç”¨ embeddings çš„æ¨¡å‹è¡¨ç¾å’Œæ–‡æœ¬æœ¬èº«çš„ç‰¹æ€§éå¸¸ç›¸é—œã€‚å› ç‚ºå½±è©•æ™®ééå¸¸çŸ­ã€åŒ…å«å¤§é‡å¹³å¸¸èªç‚ºçš„ stop words ä»¥åŠæ¨™é»ç¬¦è™Ÿã€è¡¨æƒ…ç¬¦è™Ÿç­‰ç­‰ï¼Œä¸åˆ© word embeddings çš„åˆ†é¡ä»»å‹™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0dc24",
   "metadata": {},
   "source": [
    "## (3) Train the classification model with training set and test set. (20%)\n",
    "\n",
    "* é¦–å…ˆï¼Œå› ç‚ºæ­Œè©è³‡æ–™çˆ¬å–æ™‚æ˜¯ç…§è‘—æ³¨éŸ³ç¬¦è™Ÿä¹‹é †åºï¼Œæ•…åœ¨åˆ† training set èˆ‡ test set ä¹‹å‰å…ˆé€²è¡Œ shuffle\n",
    "* åˆ†ç‚º training set èˆ‡ test set\n",
    "* å»ºæ§‹æ¨¡å‹ä¸¦æ”¾å…¥å‰é¢è‡ªè£½çš„ embeddings åšåˆ†é¡ä»»å‹™\n",
    "* è©•ä¼°æ–¹å¼æ¯”ç…§å‰ä¸€é¡Œä¹‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d6fdf",
   "metadata": {},
   "source": [
    "#### shuffle data ä¸¦å€åˆ† training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffae0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shuffled_feature, shuffled_label = shuffle(padded_docs, labels, random_state = 888)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(shuffled_feature, shuffled_label, test_size = 0.2, random_state = 413)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab120aef",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹è¨“ç·´ï¼ˆåŒ…å«æ¨¡å‹æ­å»ºã€åŠ å…¥å‰é¢è¨“ç·´çš„ embeddingsã€åˆ—å° model summary èˆ‡ model evaluationï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "538b8645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 948, 10)           436580    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9480)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9481      \n",
      "=================================================================\n",
      "Total params: 446,061\n",
      "Trainable params: 446,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 83.276838\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "\n",
    "model_new = Sequential()\n",
    "model_new.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "model_new.add(Flatten())\n",
    "model_new.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "\n",
    "model_new.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_new.fit(feature_train, np.array(label_train), epochs = 50, verbose = 0)\n",
    "\n",
    "\n",
    "# summarize the model\n",
    "\n",
    "print(model_new.summary())\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "loss_new, accuracy_new = model_new.evaluate(feature_test, np.array(label_test), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy_new*100))\n",
    "print('Loss: %d' % loss_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96df40a",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰é¡Œ Reflection\n",
    "\n",
    "è¨“ç·´å®Œå¾Œæœƒç™¼ç¾é›–ç„¶æ¨¡å‹çš„ accuracy é‚„ç®—é«˜ï¼Œä½†å°æ¯”å‰ä¸€é¡Œæ²’æœ‰å€åˆ†è¨“ç·´èˆ‡æ¸¬è©¦é›†çš„æ¨¡å‹ä¾†èªªä½äº†éå¸¸å¤šã€‚æˆ‘çŒœæ˜¯å› ç‚ºå¦‚æœæ²’æœ‰å€åˆ†çš„è©±è¨“ç·´çš„è³‡æ–™ä¹Ÿæœƒç­‰æ–¼è¦é æ¸¬çš„è³‡æ–™ï¼Œæ•…æ¨¡å‹å¾ˆæœ‰å¯èƒ½æœƒ overfittingï¼ˆæœ‰é»åƒæ˜¯æ‹¿è‘—ç­”æ¡ˆæ‰¾ç­”æ¡ˆçš„æ„Ÿè¦ºï¼‰ã€‚æ‰€ä»¥åœ¨åšæ¨¡å‹è¨“ç·´æ™‚å€åˆ†è¨“ç·´é›†ä»¥åŠæ¸¬è©¦é›†æ˜¯éå¸¸é‡è¦çš„ï¼Œæ¯”è¼ƒèƒ½çœ‹åˆ°æ¨¡å‹çš„çœŸæ­£è¡¨ç¾ä»¥åŠè¦æ”¹é€²çš„åœ°æ–¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb743ef",
   "metadata": {},
   "source": [
    "## (4) Find an English dataset and train a model with pre-trained GloVe embeddings. Don't update the weight during training. And split the dataset into training set and test set before training a model. (20%)\n",
    "\n",
    "* é¦–å…ˆï¼Œè®€å…¥è‹±æ–‡è³‡æ–™\n",
    "* æ­¤è³‡æ–™ç”± Kaggle å–å¾—ï¼Œç‚ºå¥ˆåŠåˆ©äºå·¥ä½œæ‹›è˜ä¹‹å»£å‘Šï¼Œå…¶ä¸­åŒ…å«çœŸå¯¦èˆ‡æ¬ºé¨™çš„å»£å‘Šå…§å®¹\n",
    "* å»£å‘Šç‚ºçœŸå¯¦ä¹‹ label ç‚º 0ï¼Œåä¹‹ç‚º 1ï¼Œæ•…æ­¤æ¬¡æ¨¡å‹åˆ†é¡ç›®çš„ç‚ºåˆ¤æ–·å»£å‘ŠçŸ¥çœŸå‡\n",
    "* é€²è¡Œæ–·è©\n",
    "* æ¥è‘—ä¸‹è¼‰ GloVe é è¨“ç·´å¥½çš„ embeddings (6B, 100 dim)\n",
    "* è£½ä½œ embeddings matrix å’Œæ¬Šé‡\n",
    "* å»ºæ§‹æ¨¡å‹ï¼Œæ–¼ç¬¬ä¸€å±¤ embeddings æ”¾å…¥ GloVe è³‡æ–™ï¼Œå°‡ trainable åƒæ•¸è¨­ç½®ç‚º Falseï¼Œé¿å…æ¯æ¬¡æ¨¡å‹è‡ªè¡Œæ›´æ–°æ¬Šé‡\n",
    "* shuffle è³‡æ–™ä¸¦å€åˆ† training and test set\n",
    "* è©•ä¼°æ¨¡å‹æ–¹å¼ç…§å‰é¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266609c",
   "metadata": {},
   "source": [
    "#### è®€å…¥è‹±æ–‡è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28a5db35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_desc</th>\n",
       "      <th>job_desc</th>\n",
       "      <th>job_requirement</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>department</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accountant</td>\n",
       "      <td>Equity Model Limited</td>\n",
       "      <td>Accounting, Auditing &amp; Finance</td>\n",
       "      <td>Compiling, analyzing, and reporting financial ...</td>\n",
       "      <td>This position is open preferably to a male can...</td>\n",
       "      <td>75,000 - 150,000</td>\n",
       "      <td>Abuja</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Law &amp; Compliance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Content Writer</td>\n",
       "      <td>CLINTON FUND (CF)</td>\n",
       "      <td>Management &amp; Business Development</td>\n",
       "      <td>Creating, improving and maintaining content to...</td>\n",
       "      <td>Bachelor's degree in Journalism, English, Com...</td>\n",
       "      <td>60,000 - 100,000</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Content Writing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accountant</td>\n",
       "      <td>Schleez Nigeria Limited</td>\n",
       "      <td>Accounting, Auditing &amp; Finance</td>\n",
       "      <td>Managing financial transactions, preparing fin...</td>\n",
       "      <td>Minimum of Bachelor's degree in Accounting or ...</td>\n",
       "      <td>Negotiable</td>\n",
       "      <td>First Floor, Left Wing, No. 49, Olowu Street, ...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>Bons Industries Limited</td>\n",
       "      <td>Marketing &amp; Communications</td>\n",
       "      <td>Understanding of the sales process and dynamics.\"</td>\n",
       "      <td>Minimum academic qualification of BSC/HND Degr...</td>\n",
       "      <td>75,000 - 150,000</td>\n",
       "      <td>Enugu</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Manufacturing &amp; Warehousing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bulk/Partnership Marketing Officer</td>\n",
       "      <td>TAMAK LOGISTICS</td>\n",
       "      <td>Marketing &amp; Communications</td>\n",
       "      <td>Establish relationships with major businesses ...</td>\n",
       "      <td>Be smart &amp; resourceful.,Great knowledge of how...</td>\n",
       "      <td>Less than 75,000</td>\n",
       "      <td>Lagos</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Shipping &amp; Logistics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            job_title             company_name  \\\n",
       "0                          Accountant     Equity Model Limited   \n",
       "1                      Content Writer        CLINTON FUND (CF)   \n",
       "2                          Accountant  Schleez Nigeria Limited   \n",
       "3                     Sales Executive  Bons Industries Limited   \n",
       "4  Bulk/Partnership Marketing Officer          TAMAK LOGISTICS   \n",
       "\n",
       "                        company_desc  \\\n",
       "0     Accounting, Auditing & Finance   \n",
       "1  Management & Business Development   \n",
       "2     Accounting, Auditing & Finance   \n",
       "3         Marketing & Communications   \n",
       "4         Marketing & Communications   \n",
       "\n",
       "                                            job_desc  \\\n",
       "0  Compiling, analyzing, and reporting financial ...   \n",
       "1  Creating, improving and maintaining content to...   \n",
       "2  Managing financial transactions, preparing fin...   \n",
       "3  Understanding of the sales process and dynamics.\"   \n",
       "4  Establish relationships with major businesses ...   \n",
       "\n",
       "                                     job_requirement            salary  \\\n",
       "0  This position is open preferably to a male can...  75,000 - 150,000   \n",
       "1   Bachelor's degree in Journalism, English, Com...  60,000 - 100,000   \n",
       "2  Minimum of Bachelor's degree in Accounting or ...        Negotiable   \n",
       "3  Minimum academic qualification of BSC/HND Degr...  75,000 - 150,000   \n",
       "4  Be smart & resourceful.,Great knowledge of how...  Less than 75,000   \n",
       "\n",
       "                                            location employment_type  \\\n",
       "0                                              Abuja       Full Time   \n",
       "1                                              Lagos       Full Time   \n",
       "2  First Floor, Left Wing, No. 49, Olowu Street, ...       Full-time   \n",
       "3                                              Enugu       Full Time   \n",
       "4                                              Lagos       Full Time   \n",
       "\n",
       "                    department  label  \n",
       "0             Law & Compliance      0  \n",
       "1              Content Writing      1  \n",
       "2                   Accounting      1  \n",
       "3  Manufacturing & Warehousing      0  \n",
       "4         Shipping & Logistics      0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_eng = pd.read_csv('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/job_posting.csv', sep=',')\n",
    "df_eng.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "117fa457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Compiling, analyzing, and reporting financial data.',\n",
       " 'Creating, improving and maintaining content to achieve our business goals. Your duties will also include sharing content to raise brand awareness and monitoring web traffic and metrics to identify best practices. ',\n",
       " 'Managing financial transactions, preparing financial reports, and providing financial advice to clients.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_desc = list(df_eng['job_desc'])\n",
    "job_desc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72f4f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_eng = list(df_eng['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a616665",
   "metadata": {},
   "source": [
    "#### è‹±æ–‡æ–·è©ã€è£½ä½œ one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "351962a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329, 208, 1, 41, 9, 116], [149, 330, 1, 35, 75, 2, 150, 13, 14, 98, 331, 332, 76, 333, 334, 335, 75, 2, 209, 336, 337, 1, 210, 338, 339, 1, 340, 2, 62, 341, 211], [36, 9, 63, 64, 9, 46, 1, 77, 9, 151, 2, 6], [342, 4, 3, 24, 58, 1, 343], [212, 17, 5, 344, 345, 346, 18, 3, 213]]\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(job_desc)\n",
    "\n",
    "vocab_size_eng = len(t.word_index) + 1\n",
    "hashed_sents = t.texts_to_sequences(job_desc)\n",
    "print(hashed_sents[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16b98da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "max_length_eng = max([len(s.split()) for s in job_desc])\n",
    "print(max_length_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e47be8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329 208   1  41   9 116   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [149 330   1  35  75   2 150  13  14  98 331 332  76 333 334 335  75   2\n",
      "  209 336 337   1 210 338 339   1 340   2  62 341 211   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 36   9  63  64   9  46   1  77   9 151   2   6   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [342   4   3  24  58   1 343   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [212  17   5 344 345 346  18   3 213   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length\n",
    "padded_sents = pad_sequences(hashed_sents, maxlen=max_length_eng, padding='post')\n",
    "print(padded_sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6ede2",
   "metadata": {},
   "source": [
    "#### å¼•å…¥ä¸‹è¼‰å¥½çš„ GloVe pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51618381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/glove/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78102f97",
   "metadata": {},
   "source": [
    "#### è£½ä½œ weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29b882c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "\n",
    "embedding_matrix = zeros((vocab_size_eng, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbd631",
   "metadata": {},
   "source": [
    "#### shuffle data ä¸¦å€åˆ† training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8e94634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shuffled_feature_eng, shuffled_label_eng = shuffle(padded_sents, labels_eng, random_state = 888)\n",
    "feature_train_eng, feature_test_eng, label_train_eng, label_test_eng = train_test_split(shuffled_feature_eng, shuffled_label_eng, test_size = 0.2, random_state = 413)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3252db",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹è¨“ç·´ï¼ˆåŒ…å«æ¨¡å‹æ­å»ºã€åŠ å…¥å‰é¢è¨“ç·´çš„ embeddingsã€åˆ—å° model summary èˆ‡ model evaluationï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16cbfbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 115, 100)          83800     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 11500)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11501     \n",
      "=================================================================\n",
      "Total params: 95,301\n",
      "Trainable params: 11,501\n",
      "Non-trainable params: 83,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 85.365856\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "model_eng = Sequential()\n",
    "e = Embedding(vocab_size_eng, 100, weights=[embedding_matrix], input_length= max_length_eng, trainable=False)\n",
    "model_eng.add(e)\n",
    "model_eng.add(Flatten())\n",
    "model_eng.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model_eng.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_eng.fit(feature_train_eng, np.array(label_train_eng), epochs = 50, verbose = 0)\n",
    "\n",
    "# summarize the model\n",
    "print(model_eng.summary())\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "loss_eng, accuracy_eng = model_eng.evaluate(feature_test_eng, np.array(label_test_eng), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy_eng*100))\n",
    "print('Loss: %d' % loss_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b156f69",
   "metadata": {},
   "source": [
    "## ç¬¬å››é¡Œ Reflection\n",
    "\n",
    "è‹±æ–‡çš„è³‡æ–™é›–ç„¶æ²’æœ‰ç”¨è‡ªè£½çš„ word embeddingsï¼Œä½†æ˜¯é¡¯ç„¶ GloVe pre-trained embeddings è¡¨ç¾è‰¯å¥½ï¼Œaccuracy é”åˆ° 85.4ï¼Œloss ç‚ºé›¶ï¼Œæ‰€ä»¥è­‰æ˜å¯ä»¥ç”¨ pre-trained embeddings é”åˆ°æƒ³è¦çš„é æ¸¬ç›®æ¨™ã€‚ä¸”é€™å€‹çš„å¥½è™•æ˜¯ä¸ç”¨æ¯æ¬¡éƒ½è¦è‡ªå·±è¨“ç·´ã€æ“”å¿ƒæœ‰ overfitting çš„å•é¡Œï¼Œå¯ä»¥æ‡‰ç”¨æ–¼ä¸åŒå…§å®¹çš„æ–‡æœ¬éƒ½æœ‰è‘—ä¸éŒ¯çš„è¡¨ç¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6565d9c",
   "metadata": {},
   "source": [
    "## BONUS-1\n",
    "### play around with the embedding visualization\n",
    "\n",
    "* é€™é‚Šè¦å¯è¦–åŒ–çš„æ˜¯ä¸­æ–‡ã€æœ‰åˆ† training and test set çš„æ¨¡å‹\n",
    "* å–å‡ºç¬¬ä¸€å±¤ embeddings çš„è³‡æ–™ï¼ˆæ¬Šé‡ï¼‰\n",
    "* åˆ—å‡ºæ‰€æœ‰ words\n",
    "* åˆ†åˆ¥å­˜æˆ Embedding Projector å¯ä»¥è®€å–ä¹‹æª”æ¡ˆ\n",
    "* å¯è¦–åŒ– (æˆæœç‚ºé™„ä»¶ä¹‹å½±ç‰‡æª”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd138e4",
   "metadata": {},
   "source": [
    "* å–å‡º embedding å±¤æ¯å€‹ checkpoint çš„ æ¬Šé‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f359b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43658, 10)\n"
     ]
    }
   ],
   "source": [
    "emb_layer = model_new.layers[0]\n",
    "weights = emb_layer.get_weights()[0]\n",
    "print(weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0ef87",
   "metadata": {},
   "source": [
    "* åˆ—å‡º word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97962ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bc6b7",
   "metadata": {},
   "source": [
    "#### åˆ†åˆ¥å­˜æˆ tsv æª”ï¼Œä»¥ä¾¿ä¸Šå‚³è‡³ Embedding Projector ä¾†é€²è¡Œå¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c52a80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "weight_vec = io.open('weight_vec.tsv', 'w', encoding='utf-8')\n",
    "word_arr = io.open('words.tsv', 'w', encoding='utf-8')\n",
    "for word in words:\n",
    "    word_vec = weights[i]\n",
    "    word_arr.write(word + \"\\n\")\n",
    "    weight_vec.write('\\t'.join([str(x) for x in word_vec]) + '\\n')\n",
    "    i=i+1\n",
    "\n",
    "weight_vec.close()\n",
    "word_arr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd50ea",
   "metadata": {},
   "source": [
    "## BONUS-2\n",
    "### play around with the GPU server and train with a larger GloVe embeddings\n",
    "\n",
    "* é€™é‚Šä¸‹è¼‰äº†æœ€å¤§çš„ GloVe embeddings (840 B, 300 dim)\n",
    "* æ¯”ç…§å‰è¿°æ–¹å¼é€²è¡Œæ¨¡å‹è¨“ç·´\n",
    "* p.s. åŸæœ¬è¦è©¦è©¦çœ‹ Word2Vec ä¹‹é¡çš„ï¼Œçµæœ gensim pacakage å’Œ numpy ç‰ˆæœ¬è¡çªï¼Œå‡ç´š numpy ä¹‹å¾Œæ› numpy å’Œ tensorflow è¡çª......åªå¥½æ”¾æ£„><"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfe96a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196007 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_index_lar = dict()\n",
    "\n",
    "with open('/Users/biaoyun/Documents/111 Spring Semester Gtaduated Institute/Data/glove.840B.300d.txt', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    \n",
    "    values = line.strip().split(' ')\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_lar[word] = coefs\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index_lar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3dbafd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "\n",
    "embedding_matrix_lar = zeros((vocab_size_eng, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector_lar = embeddings_index_lar.get(word)\n",
    "    if embedding_vector_lar is not None:\n",
    "        embedding_matrix_lar[i] = embedding_vector_lar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bbe1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 115, 300)          251400    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 34500)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 34501     \n",
      "=================================================================\n",
      "Total params: 285,901\n",
      "Trainable params: 34,501\n",
      "Non-trainable params: 251,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 80.487806\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "model_eng_lar = Sequential()\n",
    "e_lar = Embedding(vocab_size_eng, 300, weights=[embedding_matrix_lar], input_length= max_length_eng, trainable=False)\n",
    "model_eng_lar.add(e_lar)\n",
    "model_eng_lar.add(Flatten())\n",
    "model_eng_lar.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model_eng_lar.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_eng_lar.fit(feature_train_eng, np.array(label_train_eng), epochs = 50, verbose = 0)\n",
    "\n",
    "# summarize the model\n",
    "print(model_eng_lar.summary())\n",
    "\n",
    "# model evaluation\n",
    "\n",
    "loss_eng_lar, accuracy_eng_lar = model_eng_lar.evaluate(feature_test_eng, np.array(label_test_eng), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy_eng_lar*100))\n",
    "print('Loss: %d' % loss_eng_lar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d3149",
   "metadata": {},
   "source": [
    "## BONUS-2 Reflection\n",
    "\n",
    "è²»ç›¡åƒè¾›è¬å¯ä¸‹è¼‰æœ€å¤§çš„ GloVe embeddingsï¼ˆæª”æ¡ˆéå¸¸å¤§ï¼‰ï¼Œçµæœè¡¨ç¾æ²’æœ‰ 6 B 100 dim çš„ embeddings å¥½ ><ã€‚æˆ‘çŒœæ˜¯å› ç‚ºè³‡æ–™å¤ªå¤šä¸”ç¶­åº¦ä¹Ÿéå¤šï¼Œå°è‡´æ¨¡å‹åœ¨è¨“ç·´æ™‚æœ‰å¾ˆå¤šé›œè¨Šå½±éŸ¿å…¶åˆ¤æ–·ï¼Œæ‰æœƒé™ä½äº†ä¸€äº›æ•ˆæœã€‚å› æ­¤ï¼Œæˆ‘å»ºè­°åœ¨æœ¬èº« dataset æ²’æœ‰å¾ˆå¤§çš„æ™‚å€™ç”¨æ™®é€šå¤§å°çš„ word embeddings å°±è¶³å¤ äº†ã€‚å¦å¤–ï¼Œè£œå……å€‹å°è³‡è¨Šï¼Œ'glove.840B.300d.txt' è£¡é¢æœ‰ä¸æ­£å¸¸çš„ç¬¦è™Ÿï¼Œæ‰€ä»¥åœ¨è½‰æ›æˆ 32 float çš„æ™‚å€™æœƒå‡ºç¾å ±éŒ¯ï¼Œè§£æ±ºæ–¹å¼å°±æ˜¯åœ¨ readlines çš„æ™‚å€™åŠ å…¥ strip( ) æŠŠæ²’ç”¨çš„ç¬¦è™Ÿå»é™¤æ‰æ‰èƒ½å¤ æ­£å¸¸é‹è¡Œã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
